import os
from dotenv import load_dotenv
from openai import OpenAI
from fastapi import FastAPI, Request, Form
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

from langchain_community.vectorstores import FAISS

# from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import OpenAIEmbeddings

# from langchain_community.chat_models import ChatOpenAI
from langchain_openai import ChatOpenAI
from langchain.schema.runnable import RunnableLambda, RunnableMap
from langchain.prompts import PromptTemplate

# í™˜ê²½ë³€ìˆ˜ ë¡œë”©
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# FastAPI ì„¤ì •
app = FastAPI()
templates = Jinja2Templates(directory="templates")

# ë²¡í„° DB + LLM ì„¤ì •
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small", api_key=api_key)
vectorstore = FAISS.load_local(
    "faiss_kakaobank_faq_index",
    embeddings=embedding_model,
    allow_dangerous_deserialization=True,
)

# k=3ì¼ ê²½ìš°, ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œê°€ ì¶©ë¶„íˆ ì „ë‹¬ë˜ì§€ ì•Šì•„
# LLMì´ ë‹µë³€ì„ ëª»í•˜ëŠ” ê²½ìš°ê°€ ìˆì—ˆìŒ â†’ k=5ë¡œ ì¡°ì •í•˜ì—¬ ì •í™•ë„ ê°œì„ 
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
llm = ChatOpenAI(model="gpt-4o", temperature=0.1, api_key=api_key)

prompt = PromptTemplate.from_template(
    """
ë‹¹ì‹ ì€ ì¹´ì¹´ì˜¤ë±…í¬ì˜ ì¹œì ˆí•œ FAQ ì•ˆë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ 'ì°¸ê³  ìë£Œ'ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ 'ì§ˆë¬¸'ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

- ë¬¸ì„œì— ëª…í™•í•œ ê¸°ì¤€(ì˜ˆ: ë‚˜ì´, ì¡°ê±´ ë“±)ì´ ìˆë‹¤ë©´, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— í•´ë‹¹ ì—¬ë¶€ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”.
- ì£¼ê´€ì ì¸ ì¶”ì¸¡ì´ë‚˜ ë¬¸ì„œì— ì—†ëŠ” í•´ì„ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”.
- ì ˆëŒ€ ìë£Œì— ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ë‚´ì„œ ë‹µë³€í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.
- ë¬¸ì„œì—ì„œ ë‹µë³€ ê°€ëŠ¥í•œ ê·¼ê±°ê°€ ì—†ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ë‹µë³€í•˜ì„¸ìš”:

"ì£„ì†¡í•˜ì§€ë§Œ ë¬¸ì˜í•˜ì‹  ë‚´ìš©ì— ëŒ€í•œ ì •í™•í•œ ë‹µë³€ì„ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤.  
ì¹´ì¹´ì˜¤ë±…í¬ ê³ ê°ì„¼í„°ë¡œ ë¬¸ì˜í•´ì£¼ì‹œë©´ ë” ìì„¸íˆ ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."


ì§ˆë¬¸: {question}

ì°¸ê³  ìë£Œ:
{context}

ë‹µë³€:
"""
)

rag_chain = (
    RunnableMap(
        {
            "context": lambda x: "\n\n".join(
                [doc.page_content for doc in retriever.invoke(x["question"])]
            ),
            "question": lambda x: x["question"],
        }
    )
    | prompt
    | llm
)


# HTML ë Œë”ë§ ì—”ë“œí¬ì¸íŠ¸
@app.get("/", response_class=HTMLResponse)
async def serve_home(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})


# ì§ˆë¬¸ ì‘ë‹µ ì—”ë“œí¬ì¸íŠ¸
class QuestionRequest(BaseModel):
    question: str


@app.post("/ask")
async def ask_api(request: QuestionRequest):
    try:
        question = request.question
        print("ğŸ“Œ ì§ˆë¬¸:", question)
        print("ğŸ“Œ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤:")
        for doc in retriever.invoke(question):
            print("----")
            print(doc.page_content)

        result = rag_chain.invoke({"question": question})
        # print("ë‹µë³€>>>", result.content)
        return {"answer": result.content}
    except Exception as e:
        print("âŒ RAG ì²˜ë¦¬ ì˜¤ë¥˜ ë°œìƒ:", str(e))
        return JSONResponse(status_code=500, content={"error": str(e)})
